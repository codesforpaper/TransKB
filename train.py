import tensorflow as tf
import numpy as np

np.random.seed(1234)
import os
import time
import datetime
from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter
from builddata import *
# from model import ConvKB
from transformer import Transformer
from evaluation import Evaluation
from scipy.stats import rankdata

# Parameters
# ==================================================
parser = ArgumentParser("TransKB", formatter_class=ArgumentDefaultsHelpFormatter, conflict_handler='resolve')

parser.add_argument("--data", default="./data/", help="Data sources.")
parser.add_argument("--run_folder", default="", help="Data sources.")
parser.add_argument("--name", default="FB15k-237", help="Name of the dataset.")

parser.add_argument("--embedding_dim", default=100, type=int, help="Dimensionality of character embedding")
# parser.add_argument("--filter_sizes", default="1", help="Comma-separated filter sizes")
parser.add_argument("--filters", default=200, type=int, help="内层一维卷积核的数量，外层卷积核的数量应该等于embeddingSize，因为要确保每个layer后的输出维度和输入维度是一致的")
# parser.add_argument("--num_filters", default=500, type=int, help="Number of filters per filter size")
parser.add_argument("--num_heads",default=2, type=int, help="Number of the head attention")
parser.add_argument("--num_splits",default=8, type=int, help="Number of the split data")
parser.add_argument("--epsilon",default=1e-8, type=int, help="LayerNorm 层中的最小除数")
parser.add_argument("--num_blocks",default=6, type=int, help="Number of the block in Transformer")
parser.add_argument("--sequenceLength",default=3, type=int, help="Number of the sequence length in Transformer")
parser.add_argument("--dropout_keep_prob", default=0.5, type=float, help="全连接层的Dropout keep probability")
parser.add_argument("--keepProp", default=0.9, type=float, help="multi head 的Dropout keep probability")
parser.add_argument("--l2_reg_lambda", default=0.001, type=float, help="L2 regularization lambda")
parser.add_argument("--learning_rate", default=0.0001, type=float, help="Learning rate")
parser.add_argument("--is_trainable", default=True, type=bool, help="")
parser.add_argument("--batch_size", default=128, type=int, help="Batch Size")
parser.add_argument("--neg_ratio", default=1.0, type=float, help="Number of negative triples generated by positive")
parser.add_argument("--num_epochs", default=201, type=int, help="Number of training epochs")
parser.add_argument("--saveStep", default=20, type=int, help="")
parser.add_argument("--eval_freq", default=100, type=int, help="evaluate frequency ")
parser.add_argument("--print_freq", default=20, type=int, help="print frequency ")
parser.add_argument("--allow_soft_placement", default=True, type=bool, help="Allow device soft device placement")
parser.add_argument("--log_device_placement", default=False, type=bool, help="Log placement of ops on devices")
parser.add_argument("--model_name", default='fb15k', help="")
parser.add_argument("--useConstantInit", action='store_true')

parser.add_argument("--model_index", default='200', help="")
parser.add_argument("--num_splits", default=8, type=int, help="Split the validation set into 8 parts for a faster evaluation")
parser.add_argument("--testIdx", default=1, type=int, help="From 0 to 7. Index of one of 8 parts")
parser.add_argument("--decode", action='store_false')

args = parser.parse_args()
print(args)

def fixedPositionEmbedding(batchSize, sequenceLen):
    embeddedPosition = []
    for batch in range(batchSize):
        x = []
        for step in range(sequenceLen):
            a = np.zeros(sequenceLen)
            a[step] = 1
            x.append(a)
        embeddedPosition.append(x)
    return np.array(embeddedPosition, dtype="float32")

def _positionEmbedding(batchSize, sequenceLen, embeddingSize, scope="positionEmbedding"):
        # 生成可训练的位置向量
        # batchSize = self.config.batchSize
        # sequenceLen = self.config.sequenceLength
        # embeddingSize = self.config.model.embedding_dim
        
        # 生成位置的索引，并扩张到batch中所有的样本上
        positionIndex = np.tile(np.expand_dims(range(sequenceLen), 0), [batchSize, 1])

        # 根据正弦和余弦函数来获得每个位置上的embedding的第一部分​https://github.com/Kyubyong/transformer/issues/3
        positionEmbedding = np.array([[pos / np.power(10000, (i-i%2) / embeddingSize) for i in range(embeddingSize)] 
                                      for pos in range(sequenceLen)])

        # 然后根据奇偶性分别用sin和cos函数来包装
        positionEmbedding[:, 0::2] = np.sin(positionEmbedding[:, 0::2])
        positionEmbedding[:, 1::2] = np.cos(positionEmbedding[:, 1::2])

        # 将positionEmbedding转换成tensor的格式
        positionEmbedding_ = np.array(positionEmbedding, dtype='float32')

        # 得到三维的矩阵[batchSize, sequenceLen, embeddingSize]
        # positionEmbedded = np.take(positionEmbedding_, positionIndex)
        positionEmbedded =[]
        for item in positionIndex:
        	tmp =[]
        	for id in item:
        		tmp.append(positionEmbedding_[id])
        	positionEmbedded.append(tmp)

        return np.array(positionEmbedded, dtype="float32")

# Load data
print("Loading data...")

train, valid, test, words_indexes, indexes_words, \
headTailSelector, entity2id, id2entity, relation2id, id2relation = build_data(path=args.data, name=args.name)
data_size = len(train)
train_batch = Batch_Loader(train, words_indexes, indexes_words, headTailSelector, \
                           entity2id, id2entity, relation2id, id2relation, batch_size=args.batch_size,
                           neg_ratio=args.neg_ratio)

entity_array = np.array(list(train_batch.indexes_ents.keys()))

lstEmbed = []

#Using the pre-trained embeddings.
print("Using pre-trained model.")
lstEmbed = np.empty([len(words_indexes), args.embedding_dim]).astype(np.float32)
initEnt, initRel = init_norm_Vector(args.data + args.name + '/relation2vec' + str(args.embedding_dim) + '.init',
                                    args.data + args.name + '/entity2vec' + str(args.embedding_dim) + '.init',
                                    args.embedding_dim)
for _word in words_indexes:
    if _word in relation2id:
        index = relation2id[_word]
        _ind = words_indexes[_word]
        lstEmbed[_ind] = initRel[index]
    elif _word in entity2id:
        index = entity2id[_word]
        _ind = words_indexes[_word]
        lstEmbed[_ind] = initEnt[index]
    else:
        print('*****************Error********************!')
        break
lstEmbed = np.array(lstEmbed, dtype=np.float32)
assert len(words_indexes) % (len(entity2id) + len(relation2id)) == 0

print("Loading data... finished!")

x_valid = np.array(list(valid.keys())).astype(np.int32)
y_valid = np.array(list(valid.values())).astype(np.float32)

x_test = np.array(list(test.keys())).astype(np.int32)
y_test = np.array(list(test.values())).astype(np.float32)

# Training
# ==================================================
with tf.Graph().as_default():
    tf.set_random_seed(1234)
    session_conf = tf.ConfigProto(allow_soft_placement=args.allow_soft_placement,
                                  log_device_placement=args.log_device_placement)
    session_conf.gpu_options.allow_growth = True
    sess = tf.Session(config=session_conf)
    with sess.as_default():
        global_step = tf.Variable(0, name="global_step", trainable=False)
        # cnn = ConvKB(
        #     sequence_length=x_valid.shape[1],  # 3
        #     num_classes=y_valid.shape[1],  # 1
        #     pre_trained=lstEmbed,
        #     embedding_size=args.embedding_dim,
        #     filter_sizes=list(map(int, args.filter_sizes.split(","))),
        #     num_filters=args.num_filters,
        #     vocab_size=len(words_indexes),
        #     l2_reg_lambda=args.l2_reg_lambda,
        #     is_trainable=args.is_trainable,
        #     useConstantInit=args.useConstantInit)
        Trans = Transformer(
            config = args,
            # sequenceLength=x_valid.shape[1],  # 3
            # numBlocks = args.num_blocks,
            # num_classes=y_valid.shape[1],  # 1
            pre_trained=lstEmbed
            # embeddingSize=args.embedding_dim,
            # filter_sizes=list(map(int, args.filter_sizes.split(","))),
            # num_filters=args.num_filters,
            # filters = args.filters,
            # vocab_size=len(words_indexes),
            # l2RegLambda=args.l2_reg_lambda,
            # is_trainable=args.is_trainable,
            # epsilon = args.epsilon
            # useConstantInit=args.useConstantInit
        )

        optimizer = tf.train.AdamOptimizer(learning_rate=args.learning_rate)
        # optimizer = tf.train.RMSPropOptimizer(learning_rate=args.learning_rate)
        # optimizer = tf.train.GradientDescentOptimizer(learning_rate=args.learning_rate)
        grads_and_vars = optimizer.compute_gradients(Trans.loss)
        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)

        out_dir = os.path.abspath(os.path.join(args.run_folder, "runs", args.model_name))
        print("Writing to {}\n".format(out_dir))

        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it
        checkpoint_dir = os.path.abspath(os.path.join(out_dir, "checkpoints"))
        checkpoint_prefix = os.path.join(checkpoint_dir, "model")
        if not os.path.exists(checkpoint_dir):
            os.makedirs(checkpoint_dir)

        # Initialize all variables
        sess.run(tf.global_variables_initializer())

        def train_step(x_batch, y_batch):
            """
            A single training step
            """
            hits10 = 0.0
            mrr = 0.0
            mr = 0.0
            # embeddedPosition = fixedPositionEmbedding(len(x_batch), args.sequenceLength)
            embeddedPosition = _positionEmbedding(len(x_batch), args.sequenceLength, args.embedding_dim)
            # print(len(embeddedPosition[0][0]))
            # exit()
            feed_dict = {
                Trans.inputX: x_batch,
                Trans.inputY: y_batch,
                Trans.dropoutKeepProb: args.dropout_keep_prob,
                Trans.embeddedPosition: embeddedPosition,
            }
            _, step, loss, predictions = sess.run([train_op, global_step, Trans.loss, Trans.predictions], feed_dict)
            results = np.reshape(np.array(predictions), -1)
            results_with_id = rankdata(results, method='ordinal')
            _filter = results_with_id[0]

            mr += _filter
            mrr += 1.0 / _filter
            if _filter <= 10:
                hits10 += 1
            return loss, mr, mrr, hits10

        num_batches_per_epoch = int((data_size - 1) / args.batch_size) + 1
        Evaluation = Evaluation(args, train, valid, test, entity_array, entity2id, Trans, sess)
        for epoch in range(args.num_epochs):
            for batch_num in range(num_batches_per_epoch):
                x_batch, y_batch = train_batch()
                loss,mr,mrr,hits10 = train_step(x_batch, y_batch)
                current_step = tf.train.global_step(sess, global_step)
            if epoch % args.print_freq == 0:
                print("epoch:{}, loss:{}, mr:{}, mrr:{}, hits10:{}".format(epoch,loss,mr,mrr,hits10))
            if epoch > 0:
                if epoch % args.saveStep == 0:
                    path = Trans.saver.save(sess, checkpoint_prefix, global_step=epoch)
                    print("Saved model checkpoint to {}\n".format(path))
            if (epoch >0) & (epoch % args.eval_freq == 0):
                final_result = [0,0,0]
                for i in range(args.num_splits):
                    print("Evaluating the {} batch valid data".format(i+1))
                    args.testIdx = i
                    # batchSize = int(len(x_test) / (args.num_splits - 1))
                    # print(batchSize)
                    head_res,tail_res = Evaluation.evaluation()
                    mean_result = (head_res + tail_res)/ 2
                    final_result += mean_result
                    print("Head Evaluating result  --  mr:{}, mrr:{}, hits10:{}".format(head_res[0],head_res[1],head_res[2]))
                    print("Tail Evaluating result  --  mr:{}, mrr:{}, hits10:{}".format(tail_res[0],tail_res[1],tail_res[2]))
                    print("Mean Evaluating result  --  mr:{}, mrr:{}, hits10:{}".format(mean_result[0],mean_result[1],mean_result[2]))
                final_result = final_result/args.num_splits
                print("Final Evaluating result   --    mr:{}, mrr:{}, hits10:{}".format(final_result[0],final_result[1],final_result[2]))
